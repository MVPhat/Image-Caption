# -*- coding: utf-8 -*-
"""CapDec_Evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at

# Install Dependencies
"""

!pip install transformers
!pip install pycocoevalcap
!pip install git+https://github.com/openai/CLIP.git

from google.colab import drive
drive.mount('/content/drive')

"""# Defining and Import Model"""

import gdown
import pickle, json
import requests
import random
import clip
import os
import nltk
from torch import nn
import numpy as np
import torch
import torch.nn.functional as nnf
import sys
from typing import Tuple, List, Union, Optional
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
from google.colab import files
import skimage.io as io
import PIL.Image
from IPython.display import Image
from pycocoevalcap.bleu.bleu import Bleu
import matplotlib.pyplot as plt
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice

N = type(None)
V = np.array
ARRAY = np.ndarray
ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]
VS = Union[Tuple[V, ...], List[V]]
VN = Union[V, N]
VNS = Union[VS, N]
T = torch.Tensor
TS = Union[Tuple[T, ...], List[T]]
TN = Optional[T]
TNS = Union[Tuple[TN, ...], List[TN]]
TSN = Optional[TS]
TA = Union[T, ARRAY]


D = torch.device
CPU = torch.device('cpu')


def get_device(device_id: int) -> D:
    if not torch.cuda.is_available():
        return CPU
    device_id = min(torch.cuda.device_count() - 1, device_id)
    return torch.device(f'cuda:{device_id}')


CUDA = get_device

class ClipCaptionModel(nn.Module):

    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:
        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)

    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None):
        embedding_text = self.gpt.transformer.wte(tokens)
        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)
        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)
        if labels is not None:
            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)
            labels = torch.cat((dummy_token, tokens), dim=1)
        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)
        return out

    def __init__(self):
        super(ClipCaptionModel, self).__init__()
        self.prefix_length = 40
        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')
        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]
        self.clip_project = TransformerMapper(640, self.gpt_embedding_size, 40, 40, 8)



class MLP(nn.Module):

    def forward(self, x: T) -> T:
        return self.model(x)

    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):
        super(MLP, self).__init__()
        layers = []
        for i in range(len(sizes) -1):
            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))
            if i < len(sizes) - 2:
                layers.append(act())
        self.model = nn.Sequential(*layers)



class ClipCaptionPrefix(ClipCaptionModel):

    def parameters(self, recurse: bool = True):
        return self.clip_project.parameters()

    def train(self, mode: bool = True):
        super(ClipCaptionPrefix, self).train(mode)
        self.gpt.eval()
        return self

def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,
                  entry_length=67, temperature=1., stop_token: str = '.'):

    model.eval()
    stop_token_index = tokenizer.encode(stop_token)[0]
    tokens = None
    scores = None
    device = next(model.parameters()).device
    seq_lengths = torch.ones(beam_size, device=device)
    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)
    with torch.no_grad():
        if embed is not None:
            generated = embed
        else:
            if tokens is None:
                tokens = torch.tensor(tokenizer.encode(prompt))
                tokens = tokens.unsqueeze(0).to(device)
                generated = model.gpt.transformer.wte(tokens)
        for i in range(entry_length):
            outputs = model.gpt(inputs_embeds=generated)
            logits = outputs.logits
            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)
            logits = logits.softmax(-1).log()
            if scores is None:
                scores, next_tokens = logits.topk(beam_size, -1)
                generated = generated.expand(beam_size, *generated.shape[1:])
                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)
                if tokens is None:
                    tokens = next_tokens
                else:
                    tokens = tokens.expand(beam_size, *tokens.shape[1:])
                    tokens = torch.cat((tokens, next_tokens), dim=1)
            else:
                logits[is_stopped] = -float(np.inf)
                logits[is_stopped, 0] = 0
                scores_sum = scores[:, None] + logits
                seq_lengths[~is_stopped] += 1
                scores_sum_average = scores_sum / seq_lengths[:, None]
                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)
                next_tokens_source = next_tokens // scores_sum.shape[1]
                seq_lengths = seq_lengths[next_tokens_source]
                next_tokens = next_tokens % scores_sum.shape[1]
                next_tokens = next_tokens.unsqueeze(1)
                tokens = tokens[next_tokens_source]
                tokens = torch.cat((tokens, next_tokens), dim=1)
                generated = generated[next_tokens_source]
                scores = scores_sum_average * seq_lengths
                is_stopped = is_stopped[next_tokens_source]
            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)
            generated = torch.cat((generated, next_token_embed), dim=1)
            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()
            if is_stopped.all():
                break
    scores = scores / seq_lengths
    output_list = tokens.cpu().numpy()
    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]
    order = scores.argsort(descending=True)
    output_texts = [output_texts[i] for i in order]
    return output_texts


def generate_without_beam(
        model,
        tokenizer,
        tokens=None,
        prompt=None,
        embed=None,
        entry_count=1,
        entry_length=67,  # maximum number of words
        top_p=0.8,
        temperature=1.,
        stop_token: str = '.',
):
    model.eval()
    generated_num = 0
    generated_list = []
    stop_token_index = tokenizer.encode(stop_token)[0]
    filter_value = -float("Inf")
    device = next(model.parameters()).device

    with torch.no_grad():

        for entry_idx in trange(entry_count):
            if embed is not None:
                generated = embed
            else:
                if tokens is None:
                    tokens = torch.tensor(tokenizer.encode(prompt))
                    tokens = tokens.unsqueeze(0).to(device)

                generated = model.gpt.transformer.wte(tokens)

            for i in range(entry_length):

                outputs = model.gpt(inputs_embeds=generated)
                logits = outputs.logits
                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[
                                                    ..., :-1
                                                    ].clone()
                sorted_indices_to_remove[..., 0] = 0

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[:, indices_to_remove] = filter_value
                next_token = torch.argmax(logits, -1).unsqueeze(0)
                next_token_embed = model.gpt.transformer.wte(next_token)
                if tokens is None:
                    tokens = next_token
                else:
                    tokens = torch.cat((tokens, next_token), dim=1)
                generated = torch.cat((generated, next_token_embed), dim=1)
                if stop_token_index == next_token.item():
                    break

            output_list = list(tokens.squeeze().cpu().numpy())
            output_text = tokenizer.decode(output_list)
            generated_list.append(output_text)

    return generated_list[0]

is_gpu = True #@param {type:"boolean"}

device = CUDA(0) if is_gpu else "cpu"
clip_model, preprocess = clip.load("RN50x4", device=device, jit=False)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

class MlpTransformer(nn.Module):
    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):
        super().__init__()
        out_d = out_d if out_d is not None else in_dim
        self.fc1 = nn.Linear(in_dim, h_dim)
        self.act = act
        self.fc2 = nn.Linear(h_dim, out_d)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x

class MultiHeadAttention(nn.Module):

    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim_self // num_heads
        self.scale = head_dim ** -0.5
        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)
        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)
        self.project = nn.Linear(dim_self, dim_self)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, y=None, mask=None):
        y = y if y is not None else x
        b, n, c = x.shape
        _, m, d = y.shape
        # b n h dh
        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)
        # b m 2 h dh
        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)
        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]
        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale
        if mask is not None:
            if mask.dim() == 2:
                mask = mask.unsqueeze(1)
            attention = attention.masked_fill(mask.unsqueeze(3), float("-inf"))
        attention = attention.softmax(dim=2)
        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)
        out = self.project(out)
        return out, attention


class TransformerLayer(nn.Module):

    def forward_with_attention(self, x, y=None, mask=None):
        x_, attention = self.attn(self.norm1(x), y, mask)
        x = x + x_
        x = x + self.mlp(self.norm2(x))
        return x, attention

    def forward(self, x, y=None, mask=None):
        x = x + self.attn(self.norm1(x), y, mask)[0]
        x = x + self.mlp(self.norm2(x))
        return x

    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,
                 norm_layer: nn.Module = nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim_self)
        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)
        self.norm2 = norm_layer(dim_self)
        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)


class Transformer(nn.Module):

    def forward_with_attention(self, x, y=None, mask=None):
        attentions = []
        for layer in self.layers:
            x, att = layer.forward_with_attention(x, y, mask)
            attentions.append(att)
        return x, attentions

    def forward(self, x, y=None, mask=None):
        for i, layer in enumerate(self.layers):
            if i % 2 == 0 and self.enc_dec: # cross
                x = layer(x, y)
            elif self.enc_dec:  # self
                x = layer(x, x, mask)
            else:  # self or cross
                x = layer(x, y, mask)
        return x

    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,
                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):
        super(Transformer, self).__init__()
        dim_ref = dim_ref if dim_ref is not None else dim_self
        self.enc_dec = enc_dec
        if enc_dec:
            num_layers = num_layers * 2
        layers = []
        for i in range(num_layers):
            if i % 2 == 0 and enc_dec:  # cross
                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))
            elif enc_dec:  # self
                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))
            else:  # self or cross
                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))
        self.layers = nn.ModuleList(layers)


class TransformerMapper(nn.Module):

    def forward(self, x):
        x = self.linear(x).view(x.shape[0], self.clip_length, -1)
        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)
        prefix = torch.cat((x, prefix), dim=1)
        out = self.transformer(prefix)[:, self.clip_length:]
        return out

    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):
        super(TransformerMapper, self).__init__()
        self.clip_length = clip_length
        self.transformer = Transformer(dim_embedding, 8, num_layers)
        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)
        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)

"""# Download Pre-trained Model & Dataset Annotations"""

def download_pretrained_model():
  file_ids = {
      'pretrained_model_015_weights.pt': '1FXvOouK5wbwErgG9wppCKX3zVkFBgYBQ',
      # 'pretrained_model_0_weights.pt': '1jILs6pjb9J4QJsgtfsN6CfzmKRD5DLqo',
      # 'pretrained_model_0001_weights.pt': '1AYBjfMZ1ZzLk0PMhUenXArE990Abb6uD',
      # 'pretrained_model_001_weights.pt': '1DZ4FMNBFhW5vOBD3Yfu8dKK9_9SS-qzn',
      # 'pretrained_model_1_weights.pt': '1y2hPesWYNNzsvKBT90EoBuNJekmlwybr',
      # 'pretrained_model_25_weights.pt': '1V2DGozeFNLms42uZ4SHF782WBJB64yqd',
  }

  destination_dir = './' # ./content/ in colab
  model_path = None
  for file_name, file_id in file_ids.items():
    url = f'https://drive.google.com/uc?id={file_id}'
    model_path = f'{destination_dir}{file_name}'
    gdown.download(url, model_path, quiet=False)

  return model_path

model_path = download_pretrained_model()

model = ClipCaptionModel()


model.load_state_dict(torch.load(model_path, map_location=CPU), strict=False)
model = model.eval()
device = CUDA(0) if is_gpu else "cpu"
model = model.to(device)

# test data downloading

def download_dataset_annotation():
  file_ids = { # annotations are stored at: https://drive.google.com/drive/folders/1pO55IlZYmvhQz5EkL4t_U5ct55ko6aTm?usp=sharing
      'dataset_coco.json': '1mG5Ikg82xVBSNK_sNeGrDVM-vehbzSCh',
      'dataset_flickr8k.json': '1_-d_1YFqCeita6de-NVdgrX8T00u4_ch',
      'dataset_nocaps.json': '1ufTQZwia7R4qUMFVlvpIj76MAXYTNL-y',
      'dataset_vizwiz.json': '1nQgvVJ2QMmjcwvxmPlMZgq2HniRqsMkY',
      'COCO_2017_test.json': '171pxJ8wZsOUTUa5ecOzdGuF5J0Pji1J0',
      'COCO_2014_test.json': '1ROkbW1FAyHqA4GPGlrrt0o9i4YIBj0Ix',
  }

  destination_dir = './' # ./content/ in colab

  for file_name, file_id in file_ids.items():
      url = f'https://drive.google.com/uc?id={file_id}'
      output = f'{destination_dir}{file_name}'
      gdown.download(url, output, quiet=False)

download_dataset_annotation()

"""# Pre-processing Annotation"""

# get images url
def images_url(json_file, isSubset):
    with open(json_file, 'r') as file:
      data = json.load(file)
    test_images = None
    if isSubset is False:
      test_images = [image for image in data['images'] if image.get('split') == 'test']
    else:
      test_images = [image for image in data['images']]
    return test_images

# pre-processing sentences
def preprocess_sentence(sentence):
  return sentence.replace(",", "").replace(".", "").rstrip()

"""# Generating Caption and Evaluating Model's Performance"""

def generate_caption(dataset_name, url, use_beam_search, TEST_SIZE, test_images):
  output_captions = {}
  save_full_captions = {}
  print(f'Working on {dataset_name}\n')

  with tqdm(total=TEST_SIZE, desc=f"Generating Captions") as pbar:
    for i in range(TEST_SIZE):
      image = io.imread(f"{url}{test_images[i].get('filename')}")
      pil_image = PIL.Image.fromarray(image)

      image = preprocess(pil_image).unsqueeze(0).to(device)
      with torch.no_grad():
          prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)
          prefix_embed = model.clip_project(prefix).reshape(1, 40, -1)
      if use_beam_search:
          caption_text = generate_beam(model, tokenizer, embed=prefix_embed)[0]
      else:
          caption_text = generate_without_beam(model, tokenizer, embed=prefix_embed)
      output_captions[i] = [preprocess_sentence(caption_text)]
      save_full_captions[i] = [caption_text]
      pbar.update(1)
      if TEST_SIZE == 1:
        display(pil_image)
        print(f'{caption_text}\n\n')
    return output_captions, save_full_captions

def generate_caption2(url_or_path, use_beam_search, TEST_SIZE, test_images):
    output_captions = {}

    for i in range(TEST_SIZE):
        if url_or_path.startswith("http"):
            image = io.imread(f"{url_or_path}{test_images[i].get('file_name')}")
        else:
            image = io.imread(test_images[i])

        pil_image = PIL.Image.fromarray(image)

        image = preprocess(pil_image).unsqueeze(0).to(device)
        with torch.no_grad():
            prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)
            prefix_embed = model.clip_project(prefix).reshape(1, 40, -1)
        if use_beam_search:
            caption_text = generate_beam(model, tokenizer, embed=prefix_embed)[0]
        else:
            caption_text = generate_without_beam(model, tokenizer, embed=prefix_embed)
        output_captions[i] = [caption_text]

        display(pil_image)
        print(f'{caption_text}\n\n')

# get ref captions
def ground_truth_captions(test_images):
  tokens = {}
  for i in range(len(test_images)):
    tokens[i] = []
    for sentence in test_images[i].get('sentences'):
        tokens[i].append(sentence['raw'])
        tokens[i] = [preprocess_sentence(sentence) for sentence in tokens[i]]
  return tokens

class MetricsEvaluation():
  def __init__(self, model_name, score={}):
    self.model_name = model_name
    self.score = score

# scoring metrics
def metrics_calculation(reference_sentences, captions_from_model):
    n = 4
    scorer_bleu = Bleu(n)
    #score_meteor = Meteor()
    scorer_cider = Cider()
    scorer_rouge = Rouge()
    scorer_spice = Spice()

    #captions_from_model = {key: [value] for key, value in captions_from_model.items()}

    bleu_score, _ = scorer_bleu.compute_score(reference_sentences, captions_from_model)
    #_meteor_score, _ = score_meteor.compute_score(reference_sentences, captions_from_model)
    cider_score, _ = scorer_cider.compute_score(reference_sentences, captions_from_model)
    rouge_score, _ = scorer_rouge.compute_score(reference_sentences, captions_from_model)
    spice_score, _ = scorer_rouge.compute_score(reference_sentences, captions_from_model)

    score = {
        'BLEU-1': round(bleu_score[0], 3),
        'BLEU-2': round(bleu_score[1], 3),
        'BLEU-3': round(bleu_score[2], 3),
        'BLEU-4': round(bleu_score[3], 3),
        #'METEOR': round(_meteor_score, 3),
        'CIDEr': round(cider_score, 3),
        'ROUGE': round(rouge_score, 3),
        'SPICE': round(spice_score, 3)
    }
    return score

def visualize_evaluation(score):
  metric_names = list(score.score.keys())
  metric_scores = list(score.score.values())

  plt.figure(figsize=(10, 8))
  plt.bar(metric_names, metric_scores, color='skyblue')
  plt.xlabel('Metrics')
  plt.ylabel('Scores')
  plt.title(f'CapDec with {score.model_name}')
  plt.ylim(0, 1)
  plt.tight_layout()
  plt.show()

def delete_unavailable_images(test_images, url):
  print(f'Since there are some images in Flickr unavailable to open, we will remove them, please patient!')
  new_test_set = []
  with tqdm(total=len(test_images), desc=f"Processing") as pbar:
    for i, image in enumerate(test_images):
      image_url = f'{url}{image.get("filename")}'
      response = requests.head(image_url)
      if response.status_code == 200:
        new_test_set.append(test_images[i])
      pbar.update(1)
  return new_test_set

import time
def exp_evaluation(json_file, isSubset, dataset_name, url):
  test_images = images_url(json_file, isSubset)
  if 'Flickr' in dataset_name:
    test_images = delete_unavailable_images(test_images, url)
    time.sleep(10)
  reference_sentences = ground_truth_captions(test_images)
  captions_from_model, save_full_captions = generate_caption(dataset_name, url, True, len(test_images), test_images)

  score = MetricsEvaluation(f"0.015 variance on {dataset_name}", metrics_calculation(reference_sentences, captions_from_model))

  print(f'\nScoring for {score.model_name}\n{score.score}')
  visualize_evaluation(score)

  return test_images, save_full_captions, score

class ExperimentTest():
  def __init__(self, json_file, isSubset, dataset_name, url):
    self.json_file = json_file
    self.isSubset = isSubset
    self.dataset_name = dataset_name
    self.url = url

  def evaluate(self):
    self.test_images, self.captions_from_model, self.score = exp_evaluation(
        self.json_file,
        self.isSubset,
        self.dataset_name,
        self.url
    )

  def save_result(self):
    image_captioning = []
    for i in range(len(self.test_images)):
      img_data = {
        "url": f'{self.url}{self.test_images[i].get("filename")}',
        "caption": self.captions_from_model[i][0]
      }
      image_captioning.append(img_data)

    data = {
      "noise_variance": self.score.model_name.split()[0],
      "scores": self.score.score,
      "image_captioning": image_captioning
    }

    directory = './drive/MyDrive/CS412-Computer_Vision/evaluation/'
    if not os.path.exists(directory):
      os.makedirs(directory)

    filename = f"{self.dataset_name}.json"
    with open(f'{directory}{filename}', 'w') as file:
        json.dump(data, file, indent=4)

"""# Evaluating on different datasets with best model (0.015 noise variance)

## COCO 2014 test set with CapDec 0.015 variance. (Same dataset as training set)
"""

COCO_exp = ExperimentTest('./dataset_coco.json', False, 'COCO_test_set', 'http://images.cocodataset.org/val2014/')
COCO_exp.evaluate()
COCO_exp.save_result()

"""## Flickr8k test set with CapDec 0.015 variance."""

Flickr8k_exp = ExperimentTest('./dataset_flickr8k.json', False, 'Flickr8k_test_set', 'https://farm4.static.flickr.com/3221/')
Flickr8k_exp.evaluate()
Flickr8k_exp.save_result()

"""## Nocaps test set with CapDec 0.015 variance."""

Nocaps_exp = ExperimentTest('./dataset_nocaps.json', True, 'Nocaps_test_set', 'https://s3.amazonaws.com/nocaps/val/')
Nocaps_exp.evaluate()
Nocaps_exp.save_result()

"""## VizWiz test set with CapDec 0.015 variance."""

VizWiz_exp = ExperimentTest('./dataset_vizwiz.json', True, 'VizWiz_test_set', 'https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/')
VizWiz_exp.evaluate()
VizWiz_exp.save_result()

"""# Evalutating by ourself

### Select some images from `COCO 2014 test set` and `COCO 2017 test set` to assess the accuracy of captions
"""

with open('./COCO_2014_test.json', 'r') as file:
  COCO_2014 = json.load(file)['images']

random.shuffle(COCO_2014)
COCO_2014 = COCO_2014[:30] # select randomly 30 images from test set

generate_caption2('http://images.cocodataset.org/test2014/', True, len(COCO_2014), COCO_2014)

with open('./COCO_2017_test.json', 'r') as file:
  COCO_2017 = json.load(file)['images']

random.shuffle(COCO_2017)
COCO_2017 = COCO_2017[:30] # select randomly 30 images from test set

generate_caption2('http://images.cocodataset.org/test2017/', True, len(COCO_2017), COCO_2017)

"""### Upload your own images"""

device_or_url = 'Device' #@param ["Device", "URL"]

noimage = 0
my_images = []

while True:
  if device_or_url == 'Device':
    print(f"\nUpload image {noimage + 1} or cancel")
    uploaded = files.upload()
    if not uploaded:
      own_image = ''
      break
    elif len(uploaded) == 1:
      own_image = list(uploaded.keys())[0]
      my_images.append(own_image)
      noimage = noimage + 1
    else:
      print('Please upload an image!')
  else:
    url = input(f'(0 to exit) URL of image {noimage + 1}: ')
    if url == '0':
      break
    my_images.append(url)
    noimage = noimage + 1


generate_caption2('', False, len(my_images), my_images)